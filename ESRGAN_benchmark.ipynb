{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7cHmhUmaZ_-",
        "outputId": "b0dea3c8-c8c2-4382-e453-222946f3ec38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Dataset.zip'\n",
        "extract_path = '/content/data/test'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ],
      "metadata": {
        "id": "AYM8pkNqacvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HJggRE4Wn_CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall piq completely\n",
        "!pip uninstall -y piq\n",
        "\n",
        "# Clean up any cached versions\n",
        "!rm -rf piq\n",
        "\n",
        "# Clone the latest PIQ with niqe support\n",
        "!git clone https://github.com/photosynthesis-team/piq.git\n",
        "\n",
        "# Move into the directory\n",
        "%cd piq\n",
        "\n",
        "# Install from source\n",
        "!pip install . --quiet\n",
        "\n",
        "# Return to your working directory\n",
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CtIDslbkIdZ",
        "outputId": "fe673611-619d-4e85-b61b-3ce99fc2e344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping piq as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'piq'...\n",
            "remote: Enumerating objects: 2289, done.\u001b[K\n",
            "remote: Counting objects: 100% (890/890), done.\u001b[K\n",
            "remote: Compressing objects: 100% (502/502), done.\u001b[K\n",
            "remote: Total 2289 (delta 585), reused 635 (delta 379), pack-reused 1399 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2289/2289), 3.80 MiB | 9.31 MiB/s, done.\n",
            "Resolving deltas: 100% (1473/1473), done.\n",
            "/content/piq\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for piq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from piq import LPIPS\n",
        "print(\"✅ NIQE and LPIPS imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIqlFS3xkxqP",
        "outputId": "1bc260a9-0ed4-4927-ff12-dc1fadecd463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ NIQE and LPIPS imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from math import log10\n",
        "from math import exp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as utils\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "import math\n",
        "from piq import LPIPS"
      ],
      "metadata": {
        "id": "GXf4auTJR0we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
        "\n",
        "def display_transform():\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(400),\n",
        "        CenterCrop(400),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlBXZVlKUVjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
        "    return gauss / gauss.sum()\n",
        "\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "\n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "\n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "metadata": {
        "id": "4MMUov0QTciD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "        )\n",
        "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        block8 = self.block8(block1 + block7)\n",
        "\n",
        "        return (torch.tanh(block8) + 1) / 2\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        return x + residual\n",
        "\n",
        "class UpsampleBLock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBLock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AzIQzgpMSD4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor):\n",
        "        super(TestDatasetFromFolder, self).__init__()\n",
        "        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n",
        "        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.filenames = sorted([\n",
        "            f for f in os.listdir(self.lr_path)\n",
        "            if is_image_file(f) and os.path.exists(os.path.join(self.hr_path, f))\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        lr_image = Image.open(os.path.join(self.lr_path, filename)).convert('RGB')\n",
        "        w, h = lr_image.size\n",
        "        hr_image = Image.open(os.path.join(self.hr_path, filename)).convert('RGB')\n",
        "        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n",
        "        hr_restore_img = hr_scale(lr_image)\n",
        "        return filename, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "metadata": {
        "id": "GILvJuFnSTnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPn3hw8rRrsF",
        "outputId": "6dccdd2c-6194-46f0-dd7c-d4e790b2cdca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[testing benchmark datasets]:   0%|          | 0/19 [00:00<?, ?it/s]<ipython-input-17-2dd8d89e2c5e>:30: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  lr_image = Variable(lr_image, volatile=True)\n",
            "<ipython-input-17-2dd8d89e2c5e>:31: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  hr_image = Variable(hr_image, volatile=True)\n",
            "[testing benchmark datasets]: 100%|██████████| 19/19 [00:08<00:00,  2.17it/s]\n"
          ]
        }
      ],
      "source": [
        "UPSCALE_FACTOR = 4\n",
        "MODEL_NAME = \"netG_epoch_4_162.pth\"\n",
        "\n",
        "results = {'Set5': {'psnr': [], 'ssim': [],'lpips' : []}, 'Set14': {'psnr': [], 'ssim': [],'lpips' : []}, 'BSD100': {'psnr': [], 'ssim': [],'lpips' : []},\n",
        "           'Urban100': {'psnr': [], 'ssim': [],'lpips' : []}, 'SunHays80': {'psnr': [], 'ssim': [],'lpips' : []}}\n",
        "\n",
        "results_bicubic = {'Set5': {'psnr': [], 'ssim': [],'lpips' : []}, 'Set14': {'psnr': [], 'ssim': [],'lpips' : []}, 'BSD100': {'psnr': [], 'ssim': [],'lpips' : []},\n",
        "           'Urban100': {'psnr': [], 'ssim': [],'lpips' : []}, 'SunHays80': {'psnr': [], 'ssim': [],'lpips' : []}}\n",
        "\n",
        "model = Generator(UPSCALE_FACTOR).eval()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "model.load_state_dict(torch.load('/content/epochs/' + MODEL_NAME))\n",
        "\n",
        "lpips_model = LPIPS(reduction='mean')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    lpips_model = lpips_model.cuda()\n",
        "\n",
        "test_set = TestDatasetFromFolder('/content/data/test/Dataset', upscale_factor=UPSCALE_FACTOR)\n",
        "test_loader = DataLoader(dataset=test_set, num_workers=2, batch_size=1, shuffle=False)\n",
        "test_bar = tqdm(test_loader, desc='[testing benchmark datasets]')\n",
        "\n",
        "out_path = 'benchmark_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "for image_name, lr_image, hr_restore_img, hr_image in test_bar:\n",
        "    image_name = image_name[0]\n",
        "    lr_image = Variable(lr_image, volatile=True)\n",
        "    hr_image = Variable(hr_image, volatile=True)\n",
        "    with torch.no_grad():\n",
        "        bicubic_image = F.interpolate(lr_image, scale_factor=UPSCALE_FACTOR, mode='bicubic', align_corners=True)\n",
        "    if torch.cuda.is_available():\n",
        "        lr_image = lr_image.cuda()\n",
        "        hr_image = hr_image.cuda()\n",
        "        bicubic_image = bicubic_image.cuda()\n",
        "    sr_image = model(lr_image)\n",
        "    mse = ((hr_image - sr_image) ** 2).data.mean()\n",
        "    psnr = 10 * log10(1 / mse)\n",
        "    lpips_score = lpips_model(sr_image, hr_image)\n",
        "    calculated_ssim  = ssim(sr_image, hr_image).item()\n",
        "\n",
        "    mse_bic = ((hr_image - bicubic_image) ** 2).data.mean()\n",
        "    psnr_bic = 10 * log10(1 / mse_bic)\n",
        "    lpips_bic = lpips_model(bicubic_image, hr_image)\n",
        "    ssim_bic = ssim(bicubic_image, hr_image).item()\n",
        "\n",
        "    test_images = torch.stack(\n",
        "        [display_transform()(hr_restore_img.squeeze(0)), display_transform()(hr_image.data.cpu().squeeze(0)),\n",
        "         display_transform()(sr_image.data.cpu().squeeze(0)), display_transform()(bicubic_image.data.cpu().squeeze(0))])\n",
        "    image = utils.make_grid(test_images, nrow=4, padding=5)\n",
        "    utils.save_image(image, out_path + image_name.split('.')[0] + '_psnr_sr_%.4f_ssim_sr_%.4f_psnr_bic_%.4f_ssim_bic_%.4f.' % (psnr, calculated_ssim,psnr_bic,ssim_bic) +\n",
        "                     image_name.split('.')[-1], padding=5)\n",
        "\n",
        "    # save psnr\\ssim\n",
        "    results[image_name.split('_')[0]]['psnr'].append(psnr)\n",
        "    results[image_name.split('_')[0]]['ssim'].append(calculated_ssim)\n",
        "    results[image_name.split('_')[0]]['lpips'].append(lpips_score)\n",
        "\n",
        "    results_bicubic[image_name.split('_')[0]]['psnr'].append(psnr_bic)\n",
        "    results_bicubic[image_name.split('_')[0]]['ssim'].append(ssim_bic)\n",
        "    results_bicubic[image_name.split('_')[0]]['lpips'].append(lpips_bic)\n",
        "\n",
        "out_path = 'statistics/'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "saved_results = {'psnr': [], 'ssim': [],'lpips' : []}\n",
        "for item in results.values():\n",
        "    psnr = np.array(item['psnr'])\n",
        "    calculated_ssim = np.array(item['ssim'])\n",
        "    if len(item['lpips']) > 0:\n",
        "        lpips_score = np.array([score.detach().cpu().numpy() for score in item['lpips']])\n",
        "    else:\n",
        "        lpips_score = np.array([])\n",
        "    if (len(psnr) == 0) or (len(calculated_ssim) == 0) or (len(lpips_score) == 0):\n",
        "        psnr = 'No data'\n",
        "        calculated_ssim = 'No data'\n",
        "        lpips_score = 'No data'\n",
        "    else:\n",
        "        psnr = psnr.mean()\n",
        "        calculated_ssim = calculated_ssim.mean()\n",
        "        lpips_score = lpips_score.mean()\n",
        "    saved_results['psnr'].append(psnr)\n",
        "    saved_results['ssim'].append(calculated_ssim)\n",
        "    saved_results['lpips'].append(lpips_score)\n",
        "\n",
        "data_frame = pd.DataFrame(saved_results, results.keys())\n",
        "data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_test_results.csv', index_label='DataSet')\n",
        "\n",
        "\n",
        "saved_results_bicubic = {'psnr': [], 'ssim': [],'lpips' : []}\n",
        "\n",
        "for item in results_bicubic.values():\n",
        "    psnr = np.array(item['psnr'])\n",
        "    ssim_val = np.array(item['ssim'])\n",
        "    lpips_score = np.array([score.detach().cpu().numpy() for score in item['lpips']])\n",
        "\n",
        "    if len(psnr) == 0 or len(ssim_val) == 0 or len(lpips_score) == 0:\n",
        "        psnr = 'No data'\n",
        "        ssim_val = 'No data'\n",
        "        lpips_score = 'No data'\n",
        "    else:\n",
        "        psnr = psnr.mean()\n",
        "        ssim_val = ssim_val.mean()\n",
        "        lpips_score = lpips_score.mean()\n",
        "\n",
        "    saved_results_bicubic['psnr'].append(psnr)\n",
        "    saved_results_bicubic['ssim'].append(ssim_val)\n",
        "    saved_results_bicubic['lpips'].append(lpips_score)\n",
        "\n",
        "data_frame_bicubic = pd.DataFrame(saved_results_bicubic, results_bicubic.keys())\n",
        "data_frame_bicubic.to_csv(out_path + f'srf_{UPSCALE_FACTOR}_bicubic_results.csv', index_label='DataSet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive('/content/benchmark_results', 'zip', '/content/benchmark_results')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uS-56Q-ouiRJ",
        "outputId": "62e50b1d-d8b3-4313-db5f-6359c37eb0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/benchmark_results.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}
